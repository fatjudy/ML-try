{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0yJIfw7h30J"
   },
   "source": [
    "# Introduction to Convolutional Neural Networks\n",
    "\n",
    "## Welcome!\n",
    "This exercise serves as an introduction to building, training and testing a convolutional neural network using TensorFlow.\n",
    "\n",
    "At the end of this exercise, you should be confident to use TensorFlow to train your own conv-net on image data (or spatially related data) of your choice and to analyse and interprete the output of the model.\n",
    "\n",
    "You may refer to the lecture slides that accompany this notebook, in case you need a reference point.\n",
    "\n",
    "## Preparing this notebook\n",
    "Let us start by setting up our coding environment. \n",
    "\n",
    "The first thing you might want to do is to enable GPU usage for this notebook. You can do so by going to Edit -> Notebook Settings, clicking on Hardware Accelerator and selecting GPU. This is not strictly necessary for this exercise as we are working with small datasets and small networks. However, it does help speed up computation even with what we are working with.\n",
    "\n",
    "Next, lets import all the software packages that we will need for this exercise, most importantly TensorFlow. You may do so by running the block of code below by clicking the right-arrow button at the top left of the cell. If an error occurs, the circle would turn red - get help from your instructor. If it runs fine, you should see the currently used version of TensorFlow printed out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjSdcVkKh30K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.0.0 is loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load all libraries and helper functions\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"TensorFlow version %s is loaded.\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7Przbq8h30N"
   },
   "source": [
    "## Sanity check\n",
    "Let's check that TensorFlow works correctly by creating a tensor that contains a constant string \"Hello World!\" and then printing it out to console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKlLEYyAyKTa"
   },
   "outputs": [],
   "source": [
    "hello_world = tf.constant(\"Hello World!\", name=\"sanity_check\")\n",
    "print('Prints the tensor itself: ', hello_world)\n",
    "tf.print('Prints the values of the tensor: ', hello_world, output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2ERlv06yWSR"
   },
   "source": [
    "Notice that the variable hello_world itself is a tensor. If you wish to print the values of a tensor, you could use the tf.print method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBUQLREJh30u"
   },
   "source": [
    "## Inspecting the data\n",
    "\n",
    "We will be working with the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a very popular dataset of hand-written digits.\n",
    "\n",
    "The dataset contains 70,000 images, where each image contains a single hand-written digit from 0 to 9 and is of size 28x28 pixels in greyscale. Each image also has an accompanying label: a digit (0 to 9) representing the digit that the image contains. Our aim is to build a convolutional neural network that takes as input each image, and predicts (as a single digit) what digit the image contains.\n",
    "\n",
    "The code below loads and scales the data for our use. The data is automatically grouped into inputs and labels, and split into a training and testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWLLWFVT4_jw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the mnist dataset built into TensorFlow\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(inputs_train, labels_train), (inputs_test, labels_test) = mnist.load_data()\n",
    "# Squash the training data to values that range between 0 and 1.\n",
    "inputs_train, inputs_test = inputs_train / 255.0, inputs_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7o87-bdh50y4"
   },
   "source": [
    "Let's try to visualise some of the images. Run the following code a few times to inspect multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5Kvg0yt552F"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(20,10))\n",
    "for i in range(5):\n",
    "    char_idx = np.random.randint(inputs_train.shape[0])\n",
    "    plt.subplot(150 + i + 1)\n",
    "    plt.title(\"Label is the digit %s\" % (labels_train[char_idx]))\n",
    "    plt.imshow(inputs_train[char_idx], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCdDljbX65-B"
   },
   "source": [
    "Print the shape of each of your data objects to have a better idea of how the data is structured.\n",
    "\n",
    "The TensorFlow network that we want to build will take inputs with shape (number of samples, height, width, channels). Does the shapes you see match with the required shape? If not, you may wish to use the [np.expand_dims](https://numpy.org/doc/1.18/reference/generated/numpy.expand_dims.html) function to modify the shape of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djrRULjr6Rtm"
   },
   "outputs": [],
   "source": [
    "# Print the shapes of your data objects\n",
    "\n",
    "# Modify the shapes of the relevant data objects\n",
    "# Hint: channels should be 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-OXy4bs8FUW"
   },
   "source": [
    "Once you have gotten your data into the right shape, we can now use the tf.data API to easily shuffle and batch our dataset. Inspect and run the code cell below.\n",
    "\n",
    "What batch size are we using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3luGp8bJ8FrP"
   },
   "outputs": [],
   "source": [
    "# Use tf.data to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((inputs_train, labels_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((inputs_test, labels_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BYuvCk0h30x"
   },
   "source": [
    "## Building a convolutional neural network\n",
    "Now that we have our data ready, we can switch our attention to designing and constructing our conv-net model.\n",
    "\n",
    "As mentioned above, our aim is to build a convolutional neural network that takes as input each image, and predicts what digit the image contains. Since there are 10 possible digits, we can see this as a classification task across 10 categories.\n",
    "\n",
    "There are generally speaking two ways you can build and train a model in TensorFlow. The first is to use the Keras API such as:\n",
    "*   models.Sequential()\n",
    "*   model.fit()\n",
    "*   model.evaluate()\n",
    "\n",
    "You might already be familiar with this approach. The high level Keras API is very easy to use, however it may lack the flexibility that an advanced user might require. Today we will be looking at the second more manual but also more flexible approach.\n",
    "\n",
    "The code below is an incomplete example showing how a model could be set up using this second approach. Scrutinise the code and add more layers as you wish to complete the network. You may refer to the lecture slides linked above as a guide on how to stack layers to form a conv-net.\n",
    "\n",
    "Some layers you would need are:\n",
    "*   [tf.keras.layers.Conv2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) : 2D convolutions.\n",
    "*   [tf.keras.layers.Flatten()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) : To convert the 2D structure of image data into a 1D structure suitable for classification.\n",
    "*   [tf.keras.layers.Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) : Fully connected layers to perform the final classification.\n",
    "\n",
    "Other layers you might want to use are activation function that you can include as part of your Conv2D layers, and Pooling layers. Don't forget to check if each layer would require certain input parameters.\n",
    "\n",
    "Hint: one useful method for debugging the structure of your network is to print out the shape of each layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUnsFveb-Y2y"
   },
   "outputs": [],
   "source": [
    "class MyConvNet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyConvNet, self).__init__()\n",
    "    # This function runs whenever you create an instance of your model.\n",
    "    # Since it only runs once, you should initialise all your trainable variables here. Basically any layer that contains \"weights\" in your model.\n",
    "    \n",
    "    # Example of how you would initialise a single layer\n",
    "    self.conv1 = tf.keras.layers.Conv2D(4, 3, activation='relu')\n",
    "    # self.layer2 = ...\n",
    "    # self.layer3 = ...\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    # This function runs every time you call the model: output = model(input).\n",
    "    # The first instance of x above is just a placeholder for inputs to the model.\n",
    "    # Since it runs frequently, you should NOT initialise any trainable variables here.\n",
    "    # If you do so, those weights will be re-initialised at every call, hence defeating the purpose of the training process.\n",
    "\n",
    "    # Example of how you would utilise a single layer initialised above.\n",
    "    x = self.conv1(x)\n",
    "    # x = self.layer2(x)\n",
    "    # x = self.layer3(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Create an instance of the model\n",
    "# This instantiation invokes the __init__ function of the model from above.\n",
    "# It then waits for you to call the model to execute the \"call\" function that passes data through your model.\n",
    "model = MyConvNet() \n",
    "\n",
    "# We use SparseCategoricalCrossentropy as the loss function and Adam as the gradient descent function.\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bun4bq5FEHBn"
   },
   "source": [
    "## Training and testing the model\n",
    "\n",
    "We have specified all components of our model. We can now proceed to training and testing this model.\n",
    "\n",
    "The two functions below define what happens during each training and testing step. Inspect the code as you may need to modify it later. \n",
    "\n",
    "Note that we now use the [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) method to keep track of which operations and variables are necessary in the gradient descent step. This then allows us to perform automatic gradient descent with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ySRJKCJEkb4"
   },
   "outputs": [],
   "source": [
    "# We create the x_loss and x_accuracy objects to help keep track of model performance during training. \n",
    "# The loss and accuracy values at each step of the training process are aggregated in the objects and can be printed out at the end of each training epoch.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "\n",
    "# The function train_step executes the forward and backward propagation of a single batch of images during the training process.\n",
    "# As the TensorFlow documentation puts it, when you annotate a function with tf.function, you can still call it like any other function. \n",
    "# But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "    # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n",
    "    predictions = model(images, training=True)\n",
    "    train_step_loss = loss_function(labels, predictions)\n",
    "  # Determine the gradients for each trainable variable (weights) based on the loss function\n",
    "  gradients = tape.gradient(train_step_loss, model.trainable_variables)\n",
    "  # Apply the optimiser to the gradients to perform gradient descent\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  train_loss(train_step_loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "\n",
    "# The function test_step executes the inference (forward propagation) of a single batch of testing image.\n",
    "def test_step(images, labels):\n",
    "  # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "  # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n",
    "  predictions = model(images, training=False)\n",
    "  test_step_loss = loss_function(labels, predictions)\n",
    "  test_loss(test_step_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHQBplxEHBYQ"
   },
   "source": [
    "Finally, lets train and test the model with our training script below that calls train_step and test_step from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhFmJpdiHQFh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_conv_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-9-19bdf3d400e7>:17 train_step  *\n        predictions = model(images, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:847 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-8-adf2d981581f>:20 call  *\n        x = self.conv1(x)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:812 __call__\n        self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py:177 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [32, 28, 28]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9bdcd32e6e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;31m# Perform training across the entire train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;31m# Perform testing across the entire test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 408\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-9-19bdf3d400e7>:17 train_step  *\n        predictions = model(images, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:847 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-8-adf2d981581f>:20 call  *\n        x = self.conv1(x)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:812 __call__\n        self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py:177 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [32, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "for epoch in range(max_epochs):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  # Perform training across the entire train set\n",
    "  for inputs, labels in train_ds:\n",
    "    train_step(inputs, labels)\n",
    "\n",
    "  # Perform testing across the entire test set\n",
    "  for test_inputs, test_labels in test_ds:\n",
    "    test_step(test_inputs, test_labels, confusion_matrix)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLRD20yDHfcg"
   },
   "source": [
    "If you have reached this point, congratulations! You have trained a convolutional neural network to perform classification on image data.\n",
    "\n",
    "Have a look at your training results above. How else can we improve the network, or improve the way we analyse the results?\n",
    "  \n",
    "1.   Add or remove layers from your network. How does it affect model accuracy? You should aim for a test accuracy of >98%.\n",
    "\n",
    "2.   Change the parameters in each layer. Try some extreme values such as very small or large numbers of neurons or convolutional kernels. What changes do you observe?\n",
    "\n",
    "3.   A [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is a good way of visualising classification results. Modify the test_step function above to record the model's predictions in a Confusion matrix. Then print the final matrix together with the accuracy results at the end of each epoch.\n",
    "Hints:\n",
    "   *   You could initialise your confusion matrix as a numpy array: \n",
    "   \n",
    "             confusion_matrix = np.zeros((10,10))\n",
    "   *   You may wish to convert the output of your model into a numpy array before further manipulation. You may also assume that the class which has the largest value is the predicted class: \n",
    "   \n",
    "             pred_class = np.argmax(predictions.numpy(), axis=-1)\n",
    "\n",
    "4.   Based on the confusion matrix from above, can you identify weak points in your model? How would you further improve the model or the dataset given these new observations?\n",
    "\n",
    "5.   We have so far trained the model exclusively on images that contain white handwriting against a black background. What happens when we test the model against inverted images (black handwritting, white background)? You may invert your test images by doing: \n",
    "\n",
    "         inputs_test = 1 - inputs_test\n",
    "\n",
    "6.   Given your observations from inverting your test images, what would you recommend to do to be able to train a model that is more robust to variations in input data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3younfqTzwk8"
   },
   "source": [
    "## Answers for reference \n",
    "This section contains the complete code for the exercise in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vf4Oa_RSnj6p"
   },
   "outputs": [],
   "source": [
    "# Load all libraries and helper functions\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"TensorFlow version %s is loaded.\" % tf.__version__)\n",
    "\n",
    "# Load the mnist dataset built into TensorFlow\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(inputs_train, labels_train), (inputs_test, labels_test) = mnist.load_data()\n",
    "# Squash the training data to values that range between 0 and 1.\n",
    "inputs_train, inputs_test = inputs_train / 255.0, inputs_test / 255.0\n",
    "\n",
    "# TensorFlow networks take inputs with shape (number of samples, height, width, channels)\n",
    "# The current shape for the inputs are (number of samples, height, width)\n",
    "# We therefore need to add an extra dimension at the end\n",
    "inputs_train = np.expand_dims(inputs_train, axis=-1)\n",
    "inputs_test = np.expand_dims(inputs_test, axis=-1)\n",
    "\n",
    "# Print shapes of all our data as a sanity check\n",
    "print('inputs_train shape: ',inputs_train.shape)\n",
    "print('labels_train shape: ',labels_train.shape)\n",
    "print('inputs_test shape: ',inputs_test.shape)\n",
    "print('labels_test shape: ',labels_test.shape)\n",
    "\n",
    "# Use tf.data to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((inputs_train, labels_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((inputs_test, labels_test)).batch(32)\n",
    "\n",
    "# Example of a very simple Conv Net\n",
    "class MyConvNet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    # This function runs whenever you create an instance of your model.\n",
    "    # Since it only runs once, you should initialise all your trainable variables here. Basically any layer that contains \"weights\" in your model.\n",
    "    super(MyConvNet, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    # This function runs every time you call the model: output = model(input).\n",
    "    # Since it runs frequently, you should NOT initialise any trainable variables here.\n",
    "    # If you do so, those weights will be re-initialised at every call, hence defeating the purpose of the training process.\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    return self.dense2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyConvNet()\n",
    "\n",
    "# We use SparseCategoricalCrossentropy as the loss function and Adam as the gradient descent function.\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7)\n",
    "\n",
    "# We create the x_loss and x_accuracy objects to help keep track of model performance during training. \n",
    "# The loss and accuracy values at each step of the training process are aggregated in the objects and can be printed out at the end of each training epoch.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "\n",
    "# The function train_step executes the forward and backward propagation of a single batch of images during the training process.\n",
    "# As the TensorFlow documentation puts it, when you annotate a function with tf.function, you can still call it like any other function. \n",
    "# But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "    # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n",
    "    predictions = model(images, training=True)\n",
    "    train_step_loss = loss_function(labels, predictions)\n",
    "  # Determine the gradients for each trainable variable (weights) based on the loss function\n",
    "  gradients = tape.gradient(train_step_loss, model.trainable_variables)\n",
    "  # Apply the optimiser to the gradients to perform gradient descent\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  train_loss(train_step_loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "\n",
    "# The function test_step executes the inference (forward propagation) of a single batch of testing image.\n",
    "def test_step(images, labels, confusion_matrix):\n",
    "  # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "  # It is best to include it if you are ever unsure. True during training, False during validation / testing / inference.\n",
    "  predictions = model(images, training=False)\n",
    "  test_step_loss = loss_function(labels, predictions)\n",
    "  test_loss(test_step_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "  pred_class = np.argmax(predictions.numpy(), axis=-1)\n",
    "  for i in range(labels.shape[0]):\n",
    "    confusion_matrix[labels[i],pred_class[i]] += 1\n",
    "\n",
    "\n",
    "max_epochs = 5\n",
    "for epoch in range(max_epochs):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "  confusion_matrix = np.zeros((10,10))\n",
    "\n",
    "  # Perform training across the entire train set\n",
    "  for inputs, labels in train_ds:\n",
    "    train_step(inputs, labels)\n",
    "\n",
    "  # Perform testing across the entire test set\n",
    "  for test_inputs, test_labels in test_ds:\n",
    "    test_step(test_inputs, test_labels, confusion_matrix)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "  print('Confusion matrix: rows represent labels, columns represent predictions')\n",
    "  print(np.asarray(confusion_matrix,np.int32))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Convolutional Neural Networks.ipynb",
   "provenance": [
    {
     "file_id": "1MUd5js2JoUjp27XVuitKGF2XhRcvXbWn",
     "timestamp": 1586410795392
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
